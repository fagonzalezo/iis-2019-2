{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 2: Agentes que Aprenden\n",
    "### [Introducción a los Sistemas Inteligentes 2019-1](https://fagonzalezo.github.io/iis-2019-1/)\n",
    "### Universidad Nacional de Colombia, Bogotá\n",
    "\n",
    "---\n",
    "\n",
    "**Fecha límite de entrega**: _Viernes 5 de Julio_ antes de la medianoche\n",
    "\n",
    "\n",
    "Cerciórese de reiniciar y correr el notebook en su totalidad antes de enviarlo. Verifique que todas las salidas se muestran de manera correcta.\n",
    "\n",
    "Integrantes del grupo (máximo 3):\n",
    "\n",
    "* Nombre_1 ID_1\n",
    "* Nombre_2 ID_2\n",
    "* Nombre_3 ID_3\n",
    "\n",
    "**Instrucciones de envío:**\n",
    "\n",
    "Este notebook debe enviarse a través del siguiente [File Request](https://www.dropbox.com/request/hgzbkYaErd1LPK6y5N9j) antes de la medianoche de la fecha límite. El archivo debe nombrarse como  isi-taller1-unalusername1-unalusername2-unalusername3.ipynb, donde unalusername es el nombre de usuario asignado por la universidad (incluya los nombres de usuario de todos los miembros del grupo).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este taller es construir un agente que sea capaz de aprender a jugar el juego de *Snake*:\n",
    "\n",
    "<img src=\"https://cloud.githubusercontent.com/assets/2750531/24808769/cc825424-1bc5-11e7-816f-7320f7bda2cf.gif\" alt=\"Snake snapshot\" width=\"320\"/>\n",
    "\n",
    "El agente se entrenará usando aprendizaje supervisado. Tendremos dos versiones del agente, uno con un campo de visión limitado y otro basado en características que se calculan a partir del estado del ambiente.\n",
    "\n",
    "Al igual que en el [taller 1](https://nbviewer.jupyter.org/github/fagonzalezo/iis-2019-1/blob/master/taller1.ipynb)  vamos a usar como base este [proyecto](https://github.com/YuriyGuts/snake-ai-reinforcement) desarrollado por [Yuriy Guts](https://github.com/YuriyGuts).\n",
    "\n",
    "\n",
    "\n",
    "## 1. Agente basado en un clasificador\n",
    "\n",
    "El agente contará con una visión del campo restringida a las 9 celdas a partir de la cabeza de la serpiente (exceptuando la celda que contiene la cabeza), las cuales aparecen marcadas con un recuadro amarillo en la siguiente imagen:\n",
    "\n",
    "<img src=\"snakePerception.png\" alt=\"Snake snapshot\" width=\"320\"/>\n",
    "\n",
    "### 1.a Ambiente campo de visión restringida\n",
    "\n",
    "Extienda la clase `snakeai.gameplay.environment.Environment` de tal manera que el método `get_observation()` retorne las 8 observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snakeai.gameplay.environment import Environment\n",
    "\n",
    "class RestEnvironment(Environment):\n",
    "    \"\"\"\n",
    "    Partial observation environment. Same as base class environment, overloads \n",
    "    `get_observation` so that only the cells around and in front of the snake head \n",
    "    are returned. \n",
    "    (From Environment doc): Represents the RL environment for the Snake game that implements the game logic,\n",
    "    provides rewards for the agent and keeps track of game statistics.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, verbose=0):\n",
    "        super().__init__(config, verbose)\n",
    "\n",
    "    @property\n",
    "    def observation_shape(self):\n",
    "        \"\"\" Get the shape of the state observed at each timestep. \"\"\"\n",
    "        return 8\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\" \n",
    "        Observe the state of the environment. \n",
    "        Returns a tuple of 8 elements (x0, x2, ..., x7) where xi stores the state of the\n",
    "        corresponding position:\n",
    "        x_0: front_front_left\n",
    "        x_1: front_front_center\n",
    "        x_2: front_front_right\n",
    "        x_3: front_left\n",
    "        x_4: front_center\n",
    "        x_5: front_right\n",
    "        x_6: side_left\n",
    "        x_7: side_right\n",
    "        \"\"\"\n",
    "        return (0, 0, 0, 0, 0, 0, 0)\n",
    "    \n",
    "    def show_field(self):\n",
    "        return self.field.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Generación de ejemplos de entrenamiento\n",
    "\n",
    "Para entrenar el modelo de aprendizaje supervisado que controlará al agente necesitamos ejemplos de entrenamiento. Estos ejemplos los vamos a generar con base en el agente que planea que se desarrolló en el [taller 1](https://nbviewer.jupyter.org/github/fagonzalezo/iis-2019-1/blob/master/taller1.ipynb). La idea es que mientras se ejecuta este agente recolectemos observaciones y la acción que se ejecutó a continuación. La acción que se ejecuta a continuación será nuestro atributo de clase. Las percepciones serán nuestros atributos de entrada. Para esto se sugiere modificar el `MasterMindPlanningAgent` desarrollado en el [taller 1](https://nbviewer.jupyter.org/github/fagonzalezo/iis-2019-1/blob/master/taller1.ipynb) de manera que siempre que calcule una acción (método `act`) grabe las 8 características calculadas a partir de la observación y la acción a ejecutar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollectionAgent(AgentBase):\n",
    "    \"\"\" \n",
    "    Represents a Snake agent that use planning for calculating an action and records\n",
    "    the perceptions and corresponding actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def begin_episode(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, observation, reward):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def plan(self, observation):\n",
    "        raise NotImplementedError \n",
    "\n",
    "    def end_episode(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Entrenamiento del modelo de clasificación\n",
    "\n",
    "* Divida los ejemplos recolectados en entrenamiento/validación y prueba. \n",
    "* Utilice los datos de entrenamiento/validación para explorar los parámetros de un clasificador SVM.\n",
    "* Utilice los datos de entrenamiento/validación para explorar los parámetros de un clasificador Random Forest.\n",
    "* Usando los mejores parámetros encontrados pruebe los dos tipos de modelo sobre el conjunto de prueba. Reporte diferentes métricas y matrices de confusión. Discuta los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d Agente basado en un clasificador\n",
    "\n",
    "Utilice los modelos creados en el ítem anterior para crear un agente que tome sus decisiones basado en un clasificador. Evalue el desempeño del agente en términos de la longitud máxima que alcanza en diferentes ambientes, de diferentes tamaños, con y sin obstáculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassifierBasedAgent(AgentBase):\n",
    "    \"\"\" \n",
    "    Represents a Snake agent that use planning for calculating an action and records\n",
    "    the perceptions and corresponding actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def begin_episode(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, observation, reward):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def plan(self, observation):\n",
    "        raise NotImplementedError \n",
    "\n",
    "    def end_episode(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agente basado en un clasificador con características avanzadas\n",
    "Vamos a extender el agente del ítem anterior para que use características más sofisticadas.\n",
    "\n",
    "\n",
    "- **a)**  Cree una clase `AdvEnvironment` que extienda `Environment` y modifique el método `get_observation` para que retorne diferentes características a partir del campo de juego. Ejemplos de características:\n",
    "  * Las 8 características del ítem anterior\n",
    "  * Distancia de la cabeza a la fruta\n",
    "  * Longitud de la serpiente\n",
    "  * Distancia de la cabeza a la cola\n",
    "  * etc.\n",
    "- **b)** Cree un agente `AdvDataCollectionAgent` que recolecte los ejemplos de entrenamiento.\n",
    "- **c)** Repita el punto 1.c con las nuevas características\n",
    "- **d)** Cree un agente `ClassifierBasedAgent` basado en el nuevo clasificador. Evalue el desempeño del agente en términos de la longitud máxima que alcanza en diferentes ambientes, de diferentes tamaños, con y sin obstáculos. Contraste los resultados con los obtenidos con el agente del punto anterior. Discuta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
